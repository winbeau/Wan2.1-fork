{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": "# 单层注意力分析 / Single-Layer Attention Analysis\n\n## 工作流程 / Workflow\n\n**Step 1:** 运行注意力提取脚本 / Run the attention extraction script:\n```bash\nbash experiments/extract_attention.sh ./Wan2.1-T2V-1.3B 20 cache\n```\n\n**Step 2:** 更新下方 `DATA_PATH` 指向生成的 `.pt` 文件 / Update `DATA_PATH` below\n\n**Step 3:** 运行所有 cell 生成图表 / Run all cells to generate plots\n\n---\n\n绘制两张图 / Two plots:\n1. **2D 热力图**: Query Frame × Key Frame，3×4 网格显示 12 个 head\n2. **Per-Head Grid**: 最后一个 block 对各帧的注意力柱状图"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "plt.rcParams.update({\n",
    "        \"svg.fonttype\": \"none\",\n",
    "        \"font.family\": \"sans-serif\",\n",
    "        \"font.sans-serif\": [\"Arial\", \"DejaVu Sans\", \"Helvetica\"],\n",
    "        \"font.size\": 11,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"axes.titlesize\": 13,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"figure.dpi\": 150,\n",
    "        \"savefig.dpi\": 300,\n",
    "        \"savefig.bbox\": \"tight\",\n",
    "        \"savefig.pad_inches\": 0.1,  \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-cell",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# 配置 / Configuration\n# ============================================================\n# 运行 extract_attention.sh 后，将 DATA_PATH 指向生成的 .pt 文件\n# After running extract_attention.sh, point DATA_PATH to the generated .pt file\n#\n# Example:\n#   bash experiments/extract_attention.sh ./Wan2.1-T2V-1.3B 20 cache\n#   -> generates cache/layer20.pt\n\nDATA_PATH = \"../cache/layer20.pt\"\nSAVE_DIR = \"attention_analysis\"\nSAVE_SVG = True\n\n# 检查文件是否存在\nimport os\nif not os.path.exists(DATA_PATH):\n    print(f\"⚠️  Warning: {DATA_PATH} not found!\")\n    print(\"   Run: bash experiments/extract_attention.sh <ckpt_dir> <layer_index>\")\nelse:\n    print(f\"✓ Found: {DATA_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-cell",
   "metadata": {},
   "outputs": [],
   "source": "data = torch.load(DATA_PATH, map_location=\"cpu\", weights_only=False)\n\nprint(\"=\" * 60)\nprint(f\"Layer: {data['layer_index']}\")\nprint(f\"Prompt: {data.get('prompt', 'N/A')}\")\nprint(f\"Num frames: {data.get('num_frames', 'N/A')}\")\nprint(f\"Num heads: {data.get('num_heads', 'N/A')}\")\nprint(f\"Block sizes: {data.get('block_sizes', 'N/A')}\")\nprint(f\"Last block Q frames: {data.get('last_block_query_frames', 'N/A')}\")\n\n# 加载数据\nlayer_idx = data['layer_index']\nnum_frames = data['num_frames']\nnum_heads = data['num_heads']\n\n# 更新保存目录（使用层索引）\nSAVE_DIR_LAYER = os.path.join(SAVE_DIR, f\"layer{layer_idx}\")\n\n# 完整的 frame×frame 注意力矩阵\nfull_frame_attn = data['full_frame_attention'].float().numpy()  # [num_heads, Q, K]\nprint(f\"\\nFull attention shape: {full_frame_attn.shape}\")\nprint(f\"Range: [{full_frame_attn.min():.4f}, {full_frame_attn.max():.4f}]\")\n\n# 最后一个 block 的帧注意力（用于柱状图）\nlast_block_attn = data['last_block_frame_attention'].float().numpy()  # [num_heads, K]\nlast_block_q_frames = data.get('last_block_query_frames', [num_frames-3, num_frames-2, num_frames-1])\nprint(f\"\\nLast block attention shape: {last_block_attn.shape}\")\nprint(f\"Save directory: {SAVE_DIR_LAYER}\")"
  },
  {
   "cell_type": "markdown",
   "id": "plot1-header",
   "metadata": {},
   "source": [
    "## 图1: 2D 热力图 (Query Frame × Key Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot1-cell",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# 图1: 2D 热力图 - 每个 head 一张小图\n# X-axis: Key Frame Index (0-20)\n# Y-axis: Query Frame Index (0-20)\n# Layout: 3×4 grid for 12 heads\n# ============================================================\n\nncols = 4\nnrows = math.ceil(num_heads / ncols)\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(16, 12))\naxes = axes.flatten()\n\n# 全局 colorbar 范围（对称，便于 RdBu_r 显示）\nvmax = max(abs(full_frame_attn.min()), abs(full_frame_attn.max()))\nvmin = -vmax\n\nfor h in range(num_heads):\n    ax = axes[h]\n    attn_map = full_frame_attn[h]  # [Q, K]\n    \n    im = ax.imshow(\n        attn_map,\n        cmap=\"RdBu_r\",\n        aspect=\"auto\",\n        origin=\"lower\",\n        vmin=vmin,\n        vmax=vmax,\n        interpolation=\"nearest\"\n    )\n    \n    ax.set_title(f\"Head {h}\", fontsize=11, fontweight=\"bold\")\n    ax.set_xlabel(\"Key Frame\", fontsize=9)\n    ax.set_ylabel(\"Query Frame\", fontsize=9)\n    \n    # 设置刻度\n    tick_pos = [0, num_frames // 2, num_frames - 1]\n    ax.set_xticks(tick_pos)\n    ax.set_xticklabels(tick_pos)\n    ax.set_yticks(tick_pos)\n    ax.set_yticklabels(tick_pos)\n\n# 隐藏多余的子图\nfor k in range(num_heads, len(axes)):\n    axes[k].axis(\"off\")\n\n# 添加 colorbar\nfig.subplots_adjust(right=0.92)\ncbar_ax = fig.add_axes([0.94, 0.15, 0.02, 0.7])\ncbar = fig.colorbar(im, cax=cbar_ax)\ncbar.set_label(\"Attention Logits\", fontsize=11)\n\nfig.suptitle(\n    f\"Layer {layer_idx}: 2D Attention Maps (All Heads)\\n\"\n    f\"Query Frames: 0-{num_frames-1}, Key Frames: 0-{num_frames-1}\",\n    fontsize=14,\n    fontweight=\"bold\",\n    y=1.02\n)\n\nplt.tight_layout(rect=[0, 0, 0.92, 0.98])\n\nif SAVE_SVG:\n    os.makedirs(SAVE_DIR_LAYER, exist_ok=True)\n    save_path = os.path.join(SAVE_DIR_LAYER, f\"layer{layer_idx}_2d_heatmap_all_heads.svg\")\n    plt.savefig(save_path, format=\"svg\", bbox_inches=\"tight\")\n    print(f\"Saved: {save_path}\")\n\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "plot2-header",
   "metadata": {},
   "source": [
    "## 图2: Per-Head Grid (最后一个 block 对各帧的注意力)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot2-cell",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# 图2: Per-Head Grid 柱状图\n# 显示最后一个 block (frames 18-20) 对各 key frame 的注意力\n# ============================================================\n\nkey_indices = np.arange(num_frames)\n\nncols = 4\nnrows = math.ceil(num_heads / ncols)\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(14, 3 * nrows))\naxes = axes.flatten()\n\nBAR_COLOR = sns.color_palette(\"colorblind\")[0]\n\nfor h in range(num_heads):\n    ax = axes[h]\n    head = last_block_attn[h]  # [K]\n    \n    # 计算 sink score (首帧 - 中间帧均值)\n    first = head[0]\n    middle = head[1:-1].mean() if len(head) > 2 else head.mean()\n    sink_score = first - middle\n    \n    ax.bar(key_indices, head, alpha=0.85, width=0.8, color=BAR_COLOR)\n    ax.plot(key_indices, head, \"o-\", color=\"black\", linewidth=1, markersize=2)\n    ax.set_title(f\"H{h} (sink={sink_score:.2f})\", fontsize=10, fontweight=\"bold\")\n    ax.tick_params(axis=\"both\", which=\"major\", labelsize=7)\n    ax.grid(True, alpha=0.3)\n    \n    if len(key_indices) > 10:\n        ax.set_xticks([0, len(key_indices) // 2, len(key_indices) - 1])\n\nfor k in range(num_heads, len(axes)):\n    axes[k].axis(\"off\")\n\nfig.suptitle(\n    f\"Layer {layer_idx}: Per-Head Attention Distribution\\n\"\n    f\"Query: frames {last_block_q_frames}\",\n    fontsize=12,\n    fontweight=\"bold\",\n    y=1.00 + (0.01 * nrows),\n)\n\nplt.tight_layout()\n\nif SAVE_SVG:\n    os.makedirs(SAVE_DIR_LAYER, exist_ok=True)\n    save_path = os.path.join(SAVE_DIR_LAYER, f\"layer{layer_idx}_perhead_grid.svg\")\n    plt.savefig(save_path, format=\"svg\", bbox_inches=\"tight\")\n    print(f\"Saved: {save_path}\")\n\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "stats-header",
   "metadata": {},
   "source": [
    "## 统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stats-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Layer 20 Statistics\n",
      "============================================================\n",
      "Diagonal mean (self-attention): 7.9511\n",
      "First frame mean (sink): 1.4547\n",
      "\n",
      "Per-Head Statistics (last block):\n",
      "  H 0: first=  1.787, mid=  1.454, last=  2.680, sink= +0.333\n",
      "  H 1: first=  1.240, mid=  0.186, last=  5.461, sink= +1.055\n",
      "  H 2: first=  4.270, mid=  0.990, last=  7.281, sink= +3.279\n",
      "  H 3: first=  3.805, mid=  0.452, last=  7.125, sink= +3.352\n",
      "  H 4: first=  3.789, mid=  3.106, last=  2.883, sink= +0.683\n",
      "  H 5: first=  2.662, mid=  0.583, last=  6.707, sink= +2.079\n",
      "  H 6: first=  4.168, mid=  3.730, last=  5.309, sink= +0.438\n",
      "  H 7: first=  6.180, mid=  1.992, last= 10.375, sink= +4.188\n",
      "  H 8: first=  0.476, mid= -0.379, last=  4.203, sink= +0.855\n",
      "  H 9: first=  1.579, mid=  0.468, last=  3.674, sink= +1.111\n",
      "  H10: first=  2.215, mid=  1.685, last=  3.797, sink= +0.530\n",
      "  H11: first=  1.619, mid=  0.062, last=  5.547, sink= +1.557\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(f\"Layer {layer_idx} Statistics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 全局统计\n",
    "diag = np.array([full_frame_attn[h, i, i] for h in range(num_heads) for i in range(num_frames)])\n",
    "diag_mean = diag.mean()\n",
    "first_col = full_frame_attn[:, :, 0]\n",
    "first_col_mean = first_col[first_col != 0].mean() if (first_col != 0).any() else 0\n",
    "\n",
    "print(f\"Diagonal mean (self-attention): {diag_mean:.4f}\")\n",
    "print(f\"First frame mean (sink): {first_col_mean:.4f}\")\n",
    "\n",
    "print(\"\\nPer-Head Statistics (last block):\")\n",
    "for h in range(num_heads):\n",
    "    head = last_block_attn[h]\n",
    "    first = head[0]\n",
    "    middle = head[1:-1].mean() if len(head) > 2 else head.mean()\n",
    "    last = head[-1]\n",
    "    sink = first - middle\n",
    "    \n",
    "    print(f\"  H{h:2d}: first={first:7.3f}, mid={middle:7.3f}, last={last:7.3f}, sink={sink:+7.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "wan"
version = "2.1.0"
description = "Wan: Open and Advanced Large-Scale Video Generative Models"
authors = [
    { name = "Wan Team", email = "wan.ai@alibabacloud.com" }
]
license = { file = "LICENSE.txt" }
readme = "README.md"
requires-python = ">=3.10,<3.11"
dependencies = [
    "torch>=2.4.0",
    "torchvision>=0.19.0",
    "opencv-python>=4.9.0.80",
    "diffusers>=0.31.0",
    "transformers>=4.49.0",
    "tokenizers>=0.20.3",
    "accelerate>=1.1.1",
    "tqdm",
    "imageio",
    "easydict",
    "ftfy",
    "dashscope",
    "imageio-ffmpeg",
    "flash_attn",
    "gradio>=5.0.0",
    "numpy>=1.23.5,<2",
    "huggingface-hub[cli]>=0.36.0",
]

[project.optional-dependencies]
dev = [
    "pytest",
    "black",
    "flake8",
    "isort",
    "mypy",
    "huggingface-hub[cli]"
]

[project.urls]
homepage = "https://wanxai.com"
documentation = "https://github.com/Wan-Video/Wan2.1"
repository = "https://github.com/Wan-Video/Wan2.1"
huggingface = "https://huggingface.co/Wan-AI/"
modelscope = "https://modelscope.cn/organization/Wan-AI"
discord = "https://discord.gg/p5XbdQV7"

[tool.setuptools]
packages = ["wan"]

[tool.setuptools.package-data]
"wan" = ["**/*.py"]

[tool.black]
line-length = 88

[tool.isort]
profile = "black"

[tool.mypy]
strict = true

[tool.uv]
# 1. 限制只在 Linux x86_64 下解析，避免 ARM 架构报错
environments = [
    "sys_platform == 'linux' and platform_machine == 'x86_64'",
]

# 2. 定义 PyTorch 源，但标记为 explicit（显式），不让它自动干扰其他包
[[tool.uv.index]]
name = "pytorch"
url = "https://download.pytorch.org/whl/cu121"
explicit = true

# 3. 定义 Nvidia 源
[[tool.uv.index]]
name = "nvidia"
url = "https://pypi.ngc.nvidia.com"

# 4. 【关键】强制指定 torch 相关包走 pytorch 源
[tool.uv.sources]
torch = { index = "pytorch" }
torchvision = { index = "pytorch" }
torchaudio = { index = "pytorch" }  # <-- 加上这一行
clip = { git = "https://github.com/openai/CLIP.git" }
flash-attn = { url = "https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl" }

[tool.hatch.build.targets.wheel]
only-include = ["."]

[tool.uv.extra-build-dependencies]
flash-attn = ["torch"]
